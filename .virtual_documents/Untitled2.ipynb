import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


texts = open("input.txt").read()
texts = texts[:64]
len(texts)


chunks, chunk_size = len(texts), len(texts)//4
texts = [texts[i:i+chunk_size] for i in range(0, chunks, chunk_size)]


[len(texts[i]) for i in range(len(texts))]


texts


seq_len = len(texts[0])
seq_len


batch_size = 4


chars = sorted(list(set([ch for sent in texts for ch in sent])))

itos = {i:s for i, s in enumerate(chars)}

stoi = {s:i for i, s in itos.items()}

vocab_size = len(itos)

vocab_size


def one_hot_encode(sequence, vocab_size, seq_len):
    features = np.zeros((seq_len, vocab_size), dtype=np.float32)

    for i in range(seq_len-batch_size):
        features[i, sequence[i]] = 1
    return features


class CustomDataset(Dataset):
    def __init__(self, texts):
        self.texts = texts

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        x = self.texts[idx][:-1]
        y = self.texts[idx][1:]
        x = [stoi[ch] for ch in x]
        y = [stoi[ch] for ch in y]
        x = one_hot_encode(x, vocab_size, seq_len)
        y = one_hot_encode(y, vocab_size, seq_len)
        x = torch.tensor(x)
        y = torch.tensor(y)
        return x, y


dataset = CustomDataset(texts)

dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)


for x, y in dataloader:
    print(x.shape)
    print(y.shape)


device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device")


nn.RNN


# class NeuralNetwork(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.fc1 = nn.Linear(29, 20)
#         self.fc2 = nn.Linear(20, 29)

#     def forward(self, x):
#         x = F.relu(self.fc1(x))
#         x = F.relu(self.fc2(x))
#         return x

# model = NeuralNetwork().to(device)
# print(model)

# class MyRNN(nn.Module):
#     def __init__(self, input_size, hidden_size, output_size):
#         super(MyRNN, self).__init__()
#         self.hidden_size = hidden_size
#         self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)
#         self.in2output = nn.Linear(input_size + hidden_size, output_size)

#     def forward(self, x, hidden_state):
#         combined = torch.cat((x, hidden_state), 2)
#         hidden = torch.sigmoid(self.in2hidden(combined))
#         output = self.in2output(combined)
#         return output, hidden

#     # def init_hidden(self):
#     #     return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size, device=device))

#     def init_hidden(self, batch_size):
#         hidden = torch.zeros(batch_size, seq_len, self.hidden_size, device=device)
#         return hidden

# model = MyRNN(vocab_size, 100, vocab_size).to(device)
# print(model)


# class Model(nn.Module):
#     def __init__(self, input_size, output_size, hidden_dim, n_layers):
#         super(Model, self).__init__()
#         self.hidden_dim = hidden_dim
#         self.rnn = nn.RNN(input_size, hidden_dim, 1)
#         self.fc = nn.Linear(hidden_dim, output_size)

#     def forward(self, x):
#         # batch_size = x.size(0)

#         out, hidden = self.rnn(x)

#         # getting output from the hidden state
#         out = out.view(-1, self.hidden_dim)
#         out = self.fc(out)

#         return out, hidden

#     def init_hidden(self):
#         return torch.zeros(1, self.hidden_dim)

class Model(nn.Module):
    def __init__(self, input_size, output_size, hidden_dim, n_layers):
        super(Model, self).__init__()
        self.hidden_dim = hidden_dim
        self.rnn = nn.RNN(input_size, hidden_dim, 1)
        self.fc = nn.Linear(hidden_dim, output_size)

    def forward(self, x):
        # batch_size = x.size(0)

        out, hidden = self.rnn(x)

        # getting output from the hidden state
        out = out.view(-1, self.hidden_dim)
        out = self.fc(out)

        return out, hidden

    def init_hidden(self):
        return torch.zeros(1, self.hidden_dim)

model = Model(vocab_size, vocab_size, 100, 1).to(device)
print(model)


loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)


def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()
    hidden = model.init_hidden()
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        # print(X.shape, hidden.shape)
        out, hidden = model(X)
        loss = loss_fn(out, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 1 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")


epochs = 5
for t in range(epochs):
    # print(f"Epoch {t+1}\n-------------------------------")
    train(dataloader, model, loss_fn, optimizer)
print("Done!")


hidden = model.init_hidden(1)
x = torch.randn(1, seq_len, 29, device=device)
out, hidden = model(x, hidden)

print("".join([itos[i.item()] for i in torch.argmax(out[0], dim=1)]))





import torch
import torch.nn as nn

# Load the Shakespeare dataset and create a vocabulary
with open('input.txt', 'r') as f:
    text = f.read()
chars = list(set(text))
char_to_idx = {ch:i for i,ch in enumerate(chars)}
idx_to_char = {i:ch for i,ch in enumerate(chars)}

# Define the RNN model
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x, hidden):
        out, hidden = self.rnn(x, hidden)
        out = self.fc(out)
        return out, hidden

    def init_hidden(self, batch_size):
        return torch.zeros(1, batch_size, self.hidden_size)

# Define the training parameters
input_size = len(chars)
hidden_size = 128
output_size = len(chars)
learning_rate = 0.01
num_epochs = 100

# Instantiate the model and the optimizer
model = RNN(input_size, hidden_size, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Train the model
for epoch in range(num_epochs):
    # Set the initial hidden state
    hidden = model.init_hidden(batch_size=1)
    
    # Convert the input and target sequences to one-hot encoding
    inputs = torch.tensor([char_to_idx[ch] for ch in text[:-1]], dtype=torch.long).unsqueeze(0)
    targets = torch.tensor([char_to_idx[ch] for ch in text[1:]], dtype=torch.long).unsqueeze(0)
    inputs_one_hot = torch.zeros(inputs.size(0), inputs.size(1), input_size)
    inputs_one_hot.scatter_(2, inputs.unsqueeze(2), 1)
    
    # Forward pass
    outputs, hidden = model(inputs_one_hot, hidden)
    loss = nn.functional.cross_entropy(outputs.view(-1, output_size), targets.view(-1))
    
    # Backward pass and optimization step
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Print the loss every 10 epochs
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Generate text using the trained model
start_char = 'T'
num_chars = 100
hidden = model.init_hidden(batch_size=1)
input_char = start_char
output_text = ''
with torch.no_grad():
    for i in range(num_chars):
        # Convert the input character to one-hot encoding
        input_idx = torch.tensor([char_to_idx[input_char]], dtype=torch.long)
        input_one_hot = torch.zeros(1, 1, input_size)
        input_one_hot.scatter_(2, input_idx.unsqueeze(1).unsqueeze(0), 1)
        
        # Forward pass
        output, hidden = model(input_one_hot, hidden)
        output_dist = output.squeeze().exp()
        
        # Sample the next character from the output distribution
        top_char_idx = torch.multinomial(output_dist, 1)[0]
        output_char = idx_to_char[top_char_idx.item()]
        
        # Append the output character to the output text
        output_text += output_char
        
        # Set the input character for the next iteration
        input_char = output_char
    print(output_text)




